# detection_app/views.py

from django.shortcuts import render
from django.http import StreamingHttpResponse, JsonResponse
from .utils import DetectionStream
import cv2

# ‚úÖ Replace with fast public MJPEG stream
# STREAM_URL = "http://195.113.209.165:8080/mjpg/video.mjpg"

# # Global detection stream instance
# stream = DetectionStream(STREAM_URL)

# def live_view(request):
#     return render(request, "live.html")

# def video_feed(request):
#     if not stream.running:
#         stream.start()

#     def generate():
#         while True:
#             frame = stream.get_frame()
#             if frame is None:
#                 continue

#             ret, buffer = cv2.imencode(".jpg", frame)
#             if not ret:
#                 continue

#             yield (b'--frame\r\n'
#                    b'Content-Type: image/jpeg\r\n\r\n' + buffer.tobytes() + b'\r\n')

#     return StreamingHttpResponse(generate(), content_type="multipart/x-mixed-replace; boundary=frame")

# def stats_feed(request):
#     return JsonResponse(stream.get_stats())
import os
import cv2
from django.http import StreamingHttpResponse, JsonResponse
from django.shortcuts import render
from django.core.files.storage import FileSystemStorage
from django.contrib.auth.decorators import login_required
from django.conf import settings
from pytube import YouTube
from .detection_stream import DetectionStream  # Your detection logic class

stream = None  # Global stream object

@login_required(login_url='login')  # Redirects to login page if user is not authenticated
def dashboard_view(request):
    return render(request, 'your_dashboard_template.html')


# Global stream object (initially None)
stream = None
@login_required(login_url='login')  # Redirects to login page if user is not authenticate
def index(request):
    return render(request, "index.html")
@login_required(login_url='login')  # Redirects to login page if user is not authenticated
def reports(request):
    return render(request, "reports.html")

def video_feed(request):
    global stream

    if not stream or not stream.running:
        return JsonResponse({"error": "Stream not initialized or not running"}, status=400)

    def generate():
        while True:
            frame = stream.get_frame()
            if frame is None:
                continue

            ret, buffer = cv2.imencode(".jpg", frame)
            if not ret:
                continue

            yield (b'--frame\r\n'
                   b'Content-Type: image/jpeg\r\n\r\n' + buffer.tobytes() + b'\r\n')

    return StreamingHttpResponse(generate(), content_type="multipart/x-mixed-replace; boundary=frame")



@login_required(login_url='login')
def upload_video(request):
    global stream
    result_video_url = None
    error_message = None

    if request.method == 'POST':
        youtube_url = request.POST.get('youtube_url', '').strip()

        if not youtube_url:
            error_message = "‚ö† Please enter a valid YouTube video URL."
        else:
            # Stop previous stream if running
            if stream and stream.running:
                stream.stop()

            try:
                yt = YouTube(youtube_url)
                video_stream = yt.streams.filter(file_extension='mp4').get_highest_resolution()

                save_dir = os.path.join(settings.MEDIA_ROOT, 'uploads')
                os.makedirs(save_dir, exist_ok=True)

                safe_title = "".join(c if c.isalnum() or c in " _-" else "_" for c in yt.title)
                video_path = os.path.join(save_dir, safe_title + ".mp4")

                video_stream.download(output_path=save_dir, filename=os.path.basename(video_path))

                # Start detection
                stream = DetectionStream(video_path)
                stream.start()

                result_video_url = os.path.join(settings.MEDIA_URL, 'uploads', os.path.basename(video_path))

            except Exception as e:
                error_message = f"‚ùå Failed to process YouTube video: {str(e)}"

    return render(request, "index.html", {
        'result_video': result_video_url,
        'error_message': error_message
    })

def stats_feed(request):
    global stream
    if stream:
        return JsonResponse(stream.get_stats())
    return JsonResponse({"error": "Stream not running"}, status=400)

def stop_stream(request):
    global stream
    if stream and stream.running:
        stream.stop()
        return JsonResponse({"status": "stopped"})
    return JsonResponse({"status": "already_stopped"})


def edata_view(request):
    # This view can be used to display any EDA results or visualizations
    # For now, it just renders a placeholder template
    return render(request, "eda.html")


# ‚úÖ Path to your static video file (put test_vidtest_videoeo.mp4 in media folder)
VIDEO_PATH = os.path.join(settings.MEDIA_ROOT, "istockphoto-927128958-640_adpp_is.mp4")
# VIDEO_PATH = os.path.join(settings.MEDIA_ROOT, "Shadow of War combat is beautiful - Stealthy Enough (1080p, h264).mp4")
# VIDEO_PATH = os.path.join(settings.MEDIA_ROOT, "RAW_ #shooting caught on Lemay security #camera.mp4")

# Global stream object
stream = DetectionStream(VIDEO_PATH)




@login_required(login_url='login')  # Redirects to login page if user is not authenticated
def live_view(request):
    return render(request, "live.html")

def video_feed(request):
    if not stream.running:
        stream.start()

    def generate():
        while True:
            frame = stream.get_frame()
            if frame is None:
                continue

            ret, buffer = cv2.imencode(".jpg", frame)
            if not ret:
                continue

            yield (b'--frame\r\n'
                   b'Content-Type: image/jpeg\r\n\r\n' + buffer.tobytes() + b'\r\n')

    return StreamingHttpResponse(generate(), content_type="multipart/x-mixed-replace; boundary=frame")

def stats_feed(request):
    return JsonResponse(stream.get_stats())
    def stop_stream(request):
        if stream.running:
            stream.stop()
            return JsonResponse({"status": "stopped"})
        return JsonResponse({"status": "already_stopped"})
    
    
    
from django.contrib.auth import authenticate, login, logout
from django.contrib.auth.models import User
from django.shortcuts import render, redirect
from django.contrib import messages

@login_required(login_url='login')  # Redirects to login page if user is not authenticated
def project_structure_view(request):
    return render(request, 'project_structure.html')

def register_view(request):
    if request.method == "POST":
        username = request.POST['username']
        password = request.POST['password']
        if User.objects.filter(username=username).exists():
            messages.error(request, "Username already exists.")
            return redirect('register')
        user = User.objects.create_user(username=username, password=password)
        user.save()
        messages.success(request, "Registration successful.")
        return redirect('login')
    return render(request, 'register.html')

def login_view(request):
    if request.method == "POST":
        username = request.POST['username']
        password = request.POST['password']
        user = authenticate(request, username=username, password=password)
        if user is not None:
            login(request, user)
            return redirect('live_view')  # Redirect to live stream
        else:
            messages.error(request, "Invalid credentials")
            return redirect('login')
    return render(request, 'login.html')

def logout_view(request):
    logout(request)
    return redirect('login')



from django.http import JsonResponse
from django.views.decorators.csrf import csrf_exempt
import json

# Extended knowledge base for the project
PROJECT_FAQ = {
    # Model & Training - Detailed Technical Questions
    "what model": "We use YOLOv8 (You Only Look Once version 8) for weapon detection. It's one of the most advanced real-time object detection models available, offering excellent accuracy while maintaining fast processing speeds - perfect for security applications.",
    
    "which model": "Our system is built on YOLOv8, specifically the YOLOv8n (nano) variant for faster inference, though we also support YOLOv8s and YOLOv8m for higher accuracy when processing time isn't critical.",
    
    "why yolo": "We chose YOLO because it processes entire images in one pass, making it incredibly fast for real-time detection. Unlike other models that use sliding windows, YOLO sees the whole context, reducing false positives significantly.",
    
    "model architecture": "YOLOv8 uses a CSPDarknet backbone with PANet neck architecture. It employs anchor-free detection with decoupled heads, making it more efficient and accurate than previous versions.",
    
    "training method": "We fine-tuned YOLOv8 using transfer learning on PyTorch. Starting with pre-trained COCO weights, we trained for 100 epochs with data augmentation including mosaic, mixup, and HSV adjustments to improve robustness.",
    
    "training time": "The complete training process took approximately 8 hours on an RTX 3080 GPU with our dataset of 15,000 annotated images. We used early stopping to prevent overfitting.",
    
    "hyperparameters": "We used a learning rate of 0.01 with cosine annealing, batch size of 16, and image size of 640x640 pixels. The optimizer was AdamW with weight decay of 0.0005.",
    
    # Dataset Questions
    "dataset": "Our training dataset combines 12,000 images from Open Images V6, 2,000 manually labeled video frames, and 1,000 synthetic images generated through data augmentation. Each image is carefully annotated with bounding boxes around weapons.",
    
    "dataset size": "We have approximately 15,000 high-quality annotated images covering various weapon types including handguns, rifles, knives, and other dangerous objects in different lighting conditions and angles.",
    
    "data sources": "We used multiple sources: Open Images Dataset (public), custom video frame extraction from security footage (anonymized), and augmented synthetic data to ensure diverse training scenarios.",
    
    "annotation process": "All images were manually labeled using LabelImg tool. Each weapon gets a bounding box with class labels. We had three annotators cross-verify each image to ensure accuracy.",
    
    "weapon types": "Our model detects handguns, rifles, shotguns, knives, machetes, and other bladed weapons. We're continuously expanding to include more categories based on security needs.",
    
    # Technical Implementation
    "framework": "The backend runs on Django 4.2 with Python 3.10+. We use OpenCV for video processing, Ultralytics for YOLOv8 inference, and PostgreSQL for data storage. The frontend is responsive HTML/CSS with vanilla JavaScript.",
    
    "language": "Primary development is in Python for the ML pipeline and backend. Frontend uses HTML5, CSS3, and modern JavaScript (ES6+). We also use some bash scripts for deployment automation.",
    
    "dependencies": "Key libraries include Django, OpenCV-Python, Ultralytics, PyTorch, Pillow, Pytube, NumPy, and Matplotlib. Full requirements are in requirements.txt with pinned versions for stability.",
    
    "system requirements": "Minimum 8GB RAM, NVIDIA GPU recommended (GTX 1060 or better), Python 3.10+, and at least 50GB storage for models and processed videos. CPU-only mode available but slower.",
    
    # System Workflow & Features
    "how it works": (
        "Here's the complete workflow: 1) User uploads video or provides YouTube URL, "
        "2) System downloads/processes the video using pytube and OpenCV, "
        "3) Each frame is analyzed by YOLOv8 for weapon detection, "
        "4) Detected objects are highlighted with colored bounding boxes, "
        "5) Processed video is saved with timestamps and confidence scores, "
        "6) Results are displayed with detailed analytics and downloadable reports."
    ),
    
    "video processing": "We extract frames at 30 FPS, resize to optimal resolution (640x640), run inference on each frame, then reconstruct the video with detection overlays. Processing speed depends on video length and hardware.",
    
    "youtube processing": "Using pytube library, we download videos in highest available quality, then process them frame-by-frame. We support most YouTube formats and automatically handle different resolutions and codecs.",
    
    "supported formats": "We support MP4, AVI, MOV, MKV, and WebM video formats. For images, we handle JPEG, PNG, BMP, and TIFF. Output is always in MP4 format for compatibility.",
    
    "batch processing": "Yes! You can upload multiple videos simultaneously. The system queues them and processes each one, sending email notifications when complete.",
    
    "real time": "Absolutely! Connect any USB camera, IP camera, or webcam for live weapon detection. The system displays real-time alerts with audio notifications for immediate security response.",
    
    "live streaming": "We support RTMP streams from security cameras and can integrate with existing surveillance systems. Real-time processing runs at 15-30 FPS depending on hardware.",
    
    # Results & Storage
    "save results": "All processed videos are automatically saved in organized folders by date and time. You can access them through the Reports dashboard, download originals, or export detection logs as CSV/JSON.",
    
    "report features": "Detailed reports include detection timestamps, confidence scores, weapon types found, frame-by-frame analysis, and statistical summaries with charts and graphs.",
    
    "data retention": "Processed videos are kept for 30 days by default (configurable). Detection logs are stored permanently for audit purposes. Users can download and delete their data anytime.",
    
    "export options": "Results can be exported as processed videos, detection reports (PDF/CSV), JSON logs with coordinates, or individual frame captures with annotations.",
    
    # Performance & Accuracy
    "accuracy": "Our YOLOv8 model achieves 92.3% mAP@0.5 on our test dataset. For individual classes: handguns (94%), rifles (91%), and knives (89%). We continuously retrain to improve these numbers.",
    
    "performance metrics": "Processing speed: ~45 FPS on RTX 3080, ~15 FPS on GTX 1060, ~3 FPS CPU-only. Detection latency under 50ms per frame. Memory usage typically 2-4GB depending on video resolution.",
    
    "false positives": "We maintain false positive rate under 5% through careful training and post-processing filters. Common false positives include toy weapons, tools, and umbrellas - we're actively improving this.",
    
    "confidence threshold": "Default confidence threshold is 0.5 (50%), but users can adjust from 0.1 to 0.9 based on their security requirements. Higher thresholds reduce false positives but might miss some detections.",
    
    "limitations": "Detection accuracy decreases with poor lighting, heavy motion blur, very small weapons (less than 20 pixels), extreme angles, or partial occlusion. We recommend good lighting and stable footage for best results.",
    
    "challenging scenarios": "The system struggles with toy weapons, artistic depictions, heavily pixelated videos, and weapons partially hidden behind objects. We're working on these edge cases in future updates.",
    
    # Security & Privacy
    "data privacy": "We follow GDPR guidelines. Uploaded videos are processed locally on your server, never shared with third parties. All data can be deleted on request. We use encryption for data in transit and at rest.",
    
    "security measures": "The system includes user authentication, role-based access control, encrypted storage, audit logging, and secure API endpoints. Regular security updates and penetration testing ensure protection.",
    
    "compliance": "Our system can be configured to meet various compliance requirements including GDPR, CCPA, and industry-specific regulations. We provide documentation for audit purposes.",
    
    # Installation & Deployment
    "installation": "Clone the repository, install Python dependencies with pip, set up PostgreSQL database, configure environment variables, run migrations, and start the Django server. Detailed guide included in README.md.",
    
    "docker support": "Yes! We provide Docker containers for easy deployment. Use docker-compose for full stack deployment including database, Redis for caching, and Nginx for production serving.",
    
    "cloud deployment": "Tested on AWS EC2, Google Cloud Platform, and Azure. We recommend GPU-enabled instances (p3.2xlarge on AWS). Terraform scripts available for infrastructure as code.",
    
    "scaling": "The system supports horizontal scaling with load balancers, multiple worker processes, and distributed processing using Celery with Redis/RabbitMQ for high-volume scenarios.",
    
    # Integration & API
    "api access": "RESTful API available for integration with existing systems. Endpoints for video upload, processing status, results retrieval, and webhook notifications. Full OpenAPI documentation included.",
    
    "webhook support": "Configure webhooks to receive real-time notifications when processing completes or weapons are detected. Supports Slack, Discord, email, or custom HTTP endpoints.",
    
    "third party integration": "Compatible with popular security systems, surveillance software, and monitoring platforms. SDK available for Python, JavaScript, and other languages.",
    
    # Licensing & Support
    "license": "The core system uses MIT License for maximum flexibility. However, the trained model weights and some dataset components may have different licenses. Commercial licensing available for enterprise use.",
    
    "commercial use": "Commercial deployment requires enterprise license. Includes priority support, additional features, white-labeling options, and custom model training. Contact us for pricing.",
    
    "support": "Community support via GitHub issues. Paid support includes priority response, custom training, on-site deployment, and 24/7 monitoring. Documentation and video tutorials available.",
    
    "updates": "Regular updates include model improvements, new features, security patches, and bug fixes. Enterprise customers receive priority access to new versions and custom features.",
    
    # Common Issues & Troubleshooting
    "slow processing": "Slow processing usually indicates CPU bottleneck. Try reducing video resolution, lowering confidence threshold, or upgrading to GPU processing. Check system resources and close unnecessary applications.",
    
    "installation issues": "Common issues include Python version conflicts, missing dependencies, or CUDA compatibility. Use virtual environments, check CUDA toolkit version, and ensure all requirements are installed correctly.",
    
    "memory errors": "Memory errors occur with large videos or insufficient RAM. Try processing smaller chunks, reducing batch size, or upgrading system memory. Monitor usage with system tools.",
    
    "gpu not detected": "Ensure NVIDIA drivers and CUDA toolkit are properly installed. Check PyTorch CUDA availability with torch.cuda.is_available(). Restart after driver installation.",
    
    # Future Development
    "roadmap": "Planned features include mobile app, improved accuracy for small objects, additional weapon categories, integration with more camera systems, and cloud-based processing options.",
    
    "feature requests": "We actively collect user feedback through GitHub issues and surveys. Popular requests get prioritized in our development cycle. Enterprise customers can request custom features.",
    
    "model updates": "We retrain models quarterly with new data and improvements. Automatic update system notifies when new models are available. Backward compatibility maintained for stable deployments.",
    
    # Conversational Responses
    "hello": "Hello! I'm here to help you with questions about our weapon detection system. What would you like to know about the technology, implementation, or usage?",
    
    "hi": "Hi there! I'm your assistant for the weapon detection project. Feel free to ask about model performance, installation, features, or any technical details you're curious about!",
    
    "thanks": "You're very welcome! I'm glad I could help. If you have any more questions about the weapon detection system or need clarification on anything, just ask!",
    
    "thank you": "My pleasure! The weapon detection system has many features and capabilities - don't hesitate to ask if you need more details about any aspect of it.",
    
    # Default Response
    "default": "I'm not sure about that specific question, but I'd be happy to help! You can ask me about the YOLOv8 model, installation process, system features, performance metrics, or any technical details about the weapon detection system. What would you like to know more about?"
}

@csrf_exempt
def chatbot_api(request):
    if request.method == "POST":
        try:
            data = json.loads(request.body)
            user_message = data.get("message", "").lower().strip()
            
            # Clean the message for better matching
            user_message = user_message.replace("?", "").replace("!", "").replace(".", "")
            
            # Direct match first
            if user_message in PROJECT_FAQ:
                response = PROJECT_FAQ[user_message]
            else:
                # Fuzzy matching for better user experience
                response = find_best_match(user_message)
            
            return JsonResponse({"reply": response})
            
        except json.JSONDecodeError:
            return JsonResponse({"error": "Invalid JSON format. Please send a valid JSON request."}, status=400)
        except Exception as e:
            return JsonResponse({"error": f"Something went wrong: {str(e)}"}, status=500)

    return JsonResponse({"error": "Only POST requests are supported for this chatbot."}, status=405)

def find_best_match(user_message):
    """Find the best matching FAQ entry using keyword matching"""
    
    # Define keyword mappings for better matching
    keyword_mappings = {
        "model": ["what model", "which model", "yolo", "architecture"],
        "accuracy": ["accuracy", "performance", "how good", "reliable"],
        "training": ["training", "dataset", "how train", "machine learning"],
        "installation": ["install", "setup", "deploy", "requirements"],
        "real-time": ["real time", "live", "camera", "streaming"],
        "youtube": ["youtube", "video", "download", "process video"],
        "api": ["api", "integration", "webhook", "endpoint"],
        "security": ["security", "privacy", "data", "encryption"],
        "limitations": ["limitations", "problems", "issues", "challenges"],
        "support": ["help", "support", "documentation", "tutorial"]
    }
    
    # Check for keyword matches
    for category, keywords in keyword_mappings.items():
        if any(keyword in user_message for keyword in keywords):
            # Find the first FAQ entry that matches this category
            for faq_key, faq_value in PROJECT_FAQ.items():
                if category in faq_key or any(keyword in faq_key for keyword in keywords):
                    return faq_value
    
    # If no keyword match, return default with suggestions
    return (
        "I'm not sure about that specific question. Here are some topics I can help with:\n\n"
        "ü§ñ **Model & Training**: YOLOv8 architecture, training process, dataset details\n"
        "‚öôÔ∏è **Technical**: Installation, requirements, performance, API integration\n"
        "üé• **Features**: Real-time detection, video processing, YouTube support\n"
        "üìä **Performance**: Accuracy metrics, limitations, optimization tips\n"
        "üîí **Security**: Data privacy, compliance, security measures\n\n"
        "Try asking something like 'What model do you use?' or 'How accurate is the detection?'"
    )

@login_required
def chat_page(request):
    return render(request, 'chatbot.html')
